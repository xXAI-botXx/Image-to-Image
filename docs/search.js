window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "image_to_image", "modulename": "image_to_image", "kind": "module", "doc": "<p>The Image-to-Image package consists of \nPyTorch models, run scripts (training and inference),\nPhysgen Dataloader, Testscript and evaluation notebook.\nIt also adds special losses, scheduler and more.</p>\n\n<p>Implemented models:</p>\n\n<ul>\n<li>Pix2Pix (UNet Generator with adversarial loss)</li>\n<li>PhysFormer (Transformer)</li>\n<li>ResFCN (Simple fully convolutional network - no shrinking)</li>\n<li>ResidualDesignModel (Model which consist of 2 of the other models)</li>\n</ul>\n\n<p>Use this model by running:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">sys</span>\n<span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">path</span> <span class=\"o\">+=</span> <span class=\"p\">[</span><span class=\"s2\">&quot;.&quot;</span><span class=\"p\">]</span>\n\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">image_to_image</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">main</span>\n<span class=\"n\">main</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n\n<p>Now set all your arguments/settings through the python argument. Example:</p>\n\n<pre><code>start /B python ./your_py_file_name.py ^\n  --mode train ^\n  --epochs 50 ^\n  --batch_size 1 ^\n  --lr 0.001 ^\n  --loss weighted_combined ^\n  --wc_loss_silog_lambda 0.5 ^\n  --wc_loss_weight_silog 0.1 ^\n  --wc_loss_weight_grad 5.0 ^\n  --wc_loss_weight_ssim 10.0 ^\n  --wc_loss_weight_edge_aware 5.0 ^\n  --wc_loss_weight_l1 1.0 ^\n  --wc_loss_weight_var 0.0 ^\n  --wc_loss_weight_range 0.0 ^\n  --wc_loss_weight_blur 0.0 ^\n  --optimizer adamw ^\n  --weight_decay ^\n  --weight_decay_rate 0.05 ^\n  --gradient_clipping ^\n  --gradient_clipping_threshold 2.0 ^\n  --scheduler cosine ^\n  --use_warm_up ^\n  --warm_up_start_lr 0.000005 ^\n  --warm_up_step_duration 2000 ^\n  --activate_amp ^\n  --amp_scaler grad ^\n  --checkpoint_save_dir ./checkpoints ^\n  --save_only_best_model ^\n  --validation_interval 2 ^\n  --model physicsformer ^\n  --physicsformer_in_channels 1 ^\n  --physicsformer_out_channels 1 ^\n  --physicsformer_img_size 256 ^\n  --physicsformer_patch_size 4 ^\n  --physicsformer_embedded_dim 1024 ^\n  --physicsformer_num_blocks 8 ^\n  --physicsformer_heads 16 ^\n  --physicsformer_mlp_dim 2048 ^\n  --physicsformer_dropout 0.1 ^\n  --data_variation sound_reflection ^\n  --input_type osm ^\n  --output_type standard ^\n  --device cuda ^\n  --experiment_name image-to-image ^\n  --run_name physicsformer_test ^\n  --tensorboard_path ./tensorboard ^\n  --save_path ./mlflow_images ^\n  --cmap gray ^\n  &gt; ./logs/physicsformer_test.log 2&gt;&amp;1\n</code></pre>\n\n<p>Open the folder with the evaluation notebook:</p>\n\n<div class=\"pdoc-code codehilite\">\n<pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">image_to_image</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">open_notebook_folder</span>\n<span class=\"n\">open_notebook_folder</span><span class=\"p\">()</span>\n</code></pre>\n</div>\n"}, {"fullname": "image_to_image.main", "modulename": "image_to_image.main", "kind": "module", "doc": "<p>Module to run image-to-image.<br>\nTrain, test or inference models.</p>\n\n<p>Functions:</p>\n\n<ul>\n<li>main</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.main.main", "modulename": "image_to_image.main", "qualname": "main", "kind": "function", "doc": "<p>Main entry point for image-to-image tasks.</p>\n\n<p>This function parses command-line arguments and dispatches\nthe workflow based on the specified mode:\n    - 'train': runs the training routine\n    - 'test': runs evaluation on the test dataset\n    - 'inference': runs inference on images or datasets</p>\n\n<p>Raises:</p>\n\n<ul>\n<li>ValueError: if the provided mode is unknown.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions", "modulename": "image_to_image.model_interactions", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.model_interactions.inference", "modulename": "image_to_image.model_interactions.inference", "kind": "module", "doc": "<p>Module for inferencing image to image models.</p>\n\n<p>Functions:</p>\n\n<ul>\n<li>load_image</li>\n<li>save_output</li>\n<li>inference</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.model_interactions.inference.load_image", "modulename": "image_to_image.model_interactions.inference", "qualname": "load_image", "kind": "function", "doc": "<p>Loads an image from disk, resizes it, converts to tensor, and adds a batch dimension.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>path (str): \nPath to the image file.</li>\n<li>width (int): \nDesired image width after resizing (default=256).</li>\n<li>height (int): \nDesired image height after resizing (default=256).</li>\n<li>grayscale (bool): \nWhether to load the image as grayscale (default=True).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.Tensor: Image tensor with shape [1, C, H, W], ready for model input.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">path</span>, </span><span class=\"param\"><span class=\"n\">width</span><span class=\"o\">=</span><span class=\"mi\">256</span>, </span><span class=\"param\"><span class=\"n\">height</span><span class=\"o\">=</span><span class=\"mi\">256</span>, </span><span class=\"param\"><span class=\"n\">grayscale</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.inference.save_output", "modulename": "image_to_image.model_interactions.inference", "qualname": "save_output", "kind": "function", "doc": "<p>Saves a tensor as an image file to disk.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>tensor (torch.Tensor): \nImage tensor to save. Expected shape [B, C, H, W] or [C, H, W].</li>\n<li>path (str): \nPath where the image will be saved.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span>, </span><span class=\"param\"><span class=\"n\">path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.inference.inference", "modulename": "image_to_image.model_interactions.inference", "qualname": "inference", "kind": "function", "doc": "<p>Runs inference using a pre-trained ResFCN model on a dataset or a custom image directory.</p>\n\n<p>The function supports:</p>\n\n<ul>\n<li>Loading a test dataset if no custom images are provided.</li>\n<li>Loading a pre-trained model checkpoint.</li>\n<li>Running inference on either dataset or custom images.</li>\n<li>Saving predicted outputs to the specified output directory.</li>\n</ul>\n\n<p>Parameters:</p>\n\n<ul>\n<li>args (Namespace or None): Optional argument namespace, typically from argparse.\nRequired fields in <code>args</code>:\n<ul>\n<li>device (str): Device for computation (\"cuda\" or \"cpu\").</li>\n<li>image_dir_path (str or None): Path to custom images (optional).</li>\n<li>output_dir (str): Directory where outputs will be saved.</li>\n<li>data_variation (str): Dataset variation if using dataset.</li>\n<li>input_type (str): Input type for dataset.</li>\n<li>output_type (str): Output type for dataset.</li>\n<li>fake_rgb_output (bool): Flag for dataset processing.</li>\n<li>make_14_dividable_size (bool): Flag for resizing dataset images.</li>\n<li>resfcn_in_channels (int): Number of input channels for the ResFCN model.</li>\n<li>resfcn_hidden_channels (int): Number of hidden channels for ResFCN.</li>\n<li>resfcn_out_channels (int): Number of output channels for ResFCN.</li>\n<li>resfcn_num_blocks (int): Number of residual blocks in ResFCN.</li>\n<li>model_params_path (str): Path to the saved model checkpoint.</li>\n</ul></li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>None: The function saves predicted images to disk.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.test", "modulename": "image_to_image.model_interactions.test", "kind": "module", "doc": "<p>Module for testing image to image models.</p>\n\n<p>Functions:</p>\n\n<ul>\n<li>test</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.model_interactions.test.test", "modulename": "image_to_image.model_interactions.test", "qualname": "test", "kind": "function", "doc": "<p>Runs evaluation of a pre-trained ResFCN model on a test dataset.</p>\n\n<p>This function:</p>\n\n<ul>\n<li>Loads the test dataset using <code>PhysGenDataset</code>.</li>\n<li>Loads a ResFCN model with parameters specified in <code>args</code>.</li>\n<li>Loads model weights from a checkpoint file.</li>\n<li>Evaluates the model on the test dataset using a specified loss criterion.</li>\n</ul>\n\n<p>Parameters:</p>\n\n<ul>\n<li>args (Namespace or None): Optional argument namespace, typically from argparse.\nRequired fields in <code>args</code>:\n<ul>\n<li>device (str): Device for computation (\"cuda\" or \"cpu\").</li>\n<li>data_variation (str): Dataset variation to use for testing.</li>\n<li>input_type (str): Input type for dataset.</li>\n<li>output_type (str): Output type for dataset.</li>\n<li>fake_rgb_output (bool): Flag for dataset preprocessing.</li>\n<li>make_14_dividable_size (bool): Flag for resizing dataset images.</li>\n<li>batch_size (int): Batch size for DataLoader.</li>\n<li>resfcn_in_channels (int): Number of input channels for the ResFCN model.</li>\n<li>resfcn_hidden_channels (int): Number of hidden channels for ResFCN.</li>\n<li>resfcn_out_channels (int): Number of output channels for ResFCN.</li>\n<li>resfcn_num_blocks (int): Number of residual blocks in ResFCN.</li>\n<li>model_params_path (str): Path to the saved model checkpoint.</li>\n<li>loss (str): Loss type to use for evaluation (\"l1\" or \"crossentropy\").</li>\n</ul></li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>None: Prints the test loss to stdout.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train", "modulename": "image_to_image.model_interactions.train", "kind": "module", "doc": "<p>Module to train, validate, and evaluate image-to-image models. \nSupports multiple models, losses, optimizers, schedulers, mixed-precision training,\ncheckpointing, and experiment tracking with MLflow and TensorBoard.</p>\n\n<p>The train function handles full experiment orchestration including:</p>\n\n<ul>\n<li>Argument parsing and device setup.</li>\n<li>Dataset and dataloader initialization.</li>\n<li>Model, optimizer, loss function, and scheduler setup.</li>\n<li>Mixed precision (AMP) and warm-up handling.</li>\n<li>MLflow and TensorBoard logging.</li>\n<li>Periodic validation and checkpoint saving.</li>\n</ul>\n\n<p>Functions:</p>\n\n<ul>\n<li>get_model</li>\n<li>get_loss</li>\n<li>get_optimizer</li>\n<li>get_scheduler</li>\n<li>backward_model</li>\n<li>train_one_epoch</li>\n<li>evaluate</li>\n<li>save_checkpoint</li>\n<li>train</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.model_interactions.train.get_model", "modulename": "image_to_image.model_interactions.train", "qualname": "get_model", "kind": "function", "doc": "<p>Returns a model instance based on provided arguments.</p>\n\n<p>Supported models:</p>\n\n<ul>\n<li>ResFCN</li>\n<li>Pix2Pix</li>\n<li>ResidualDesignModel</li>\n<li>PhysicFormer</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>model_name (str):\nName of the model to initialize.</li>\n<li>args:\nParsed command-line arguments.</li>\n<li>criterion:\nCriterion for Pix2Pixs second loss. Required during model initialization.</li>\n<li>device:\nTarget device (GPU or CPU) on which to place the model.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>model (nn.Module): Instantiated PyTorch model on the given device.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model_name</span>, </span><span class=\"param\"><span class=\"n\">args</span>, </span><span class=\"param\"><span class=\"n\">criterion</span>, </span><span class=\"param\"><span class=\"n\">device</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.get_loss", "modulename": "image_to_image.model_interactions.train", "qualname": "get_loss", "kind": "function", "doc": "<p>Returns a loss function instance based on the provided loss name.</p>\n\n<p>Supported losses:</p>\n\n<ul>\n<li>L1 / L1_2</li>\n<li>CrossEntropy / CrossEntropy_2</li>\n<li>WeightedCombined / WeightedCombined_2</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>loss_name (str):\nName of the loss function.</li>\n<li>args:\nParsed command-line arguments with configured loss weights.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>criterion (nn.Module): Instantiated loss function.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">loss_name</span>, </span><span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.get_optimizer", "modulename": "image_to_image.model_interactions.train", "qualname": "get_optimizer", "kind": "function", "doc": "<p>Returns an optimizer for the given model.</p>\n\n<p>Supported optimizers:</p>\n\n<ul>\n<li>Adam</li>\n<li>AdamW</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>optimizer_name (str):\nName of the optimizer.</li>\n<li>model (nn.Module):\nModel whose parameters should be optimized.</li>\n<li>lr (float):\nLearning rate.</li>\n<li>args:\nParsed command-line arguments with optimizer configuration.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>optimizer (torch.optim.Optimizer): Instantiated optimizer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">optimizer_name</span>, </span><span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">lr</span>, </span><span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.get_scheduler", "modulename": "image_to_image.model_interactions.train", "qualname": "get_scheduler", "kind": "function", "doc": "<p>Returns a learning rate scheduler for the given optimizer.</p>\n\n<p>Supported schedulers:</p>\n\n<ul>\n<li>StepLR</li>\n<li>CosineAnnealingLR</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>scheduler_name (str):\nName of the scheduler.</li>\n<li>optimizer:\nOptimizer whose learning rate will be managed.</li>\n<li>args:\nParsed command-line arguments containing scheduler configuration.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>scheduler (torch.optim.lr_scheduler): Instantiated scheduler.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">scheduler_name</span>, </span><span class=\"param\"><span class=\"n\">optimizer</span>, </span><span class=\"param\"><span class=\"n\">args</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.backward_model", "modulename": "image_to_image.model_interactions.train", "qualname": "backward_model", "kind": "function", "doc": "<p>Performs a backward pass, optimizer step, and mixed-precision handling.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>model (nn.Module):\nModel to train.</li>\n<li>x (torch.Tensor):\nInput tensor.</li>\n<li>y (torch.Tensor):\nTarget tensor.</li>\n<li>optimizer:\nOptimizer or tuple of optimizers (for Pix2Pix/ResidualDesignModel).</li>\n<li>criterion:\nLoss function.</li>\n<li>device (torch.device):\nDevice to perform computation on.</li>\n<li>epoch (int):\nCurrent training epoch.</li>\n<li>amp_scaler (GradScaler):\nGradient scaler for mixed-precision training.</li>\n<li>gradient_clipping_threshold (float, optional):\nMaximum allowed gradient norm for clipping.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>loss (torch.Tensor): Computed loss value for the batch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">y</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">epoch</span>,</span><span class=\"param\">\t<span class=\"n\">amp_scaler</span>,</span><span class=\"param\">\t<span class=\"n\">gradient_clipping_threshold</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.train_one_epoch", "modulename": "image_to_image.model_interactions.train", "qualname": "train_one_epoch", "kind": "function", "doc": "<p>Runs one full epoch of training and returns the average loss.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>model (nn.Module):\nModel to train.</li>\n<li>loader (DataLoader):\nData loader containing training batches.</li>\n<li>optimizer:\nOptimizer or tuple of optimizers.</li>\n<li>criterion:\nLoss function or tuple of losses (for multi-stage models).</li>\n<li>device (torch.device):\nDevice to use for training.</li>\n<li>epoch (int, optional):\nCurrent epoch index.</li>\n<li>amp_scaler (GradScaler, optional):\nMixed-precision scaler.</li>\n<li>gradient_clipping_threshold (float, optional):\nMax allowed gradient norm.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>avg_loss (float): Average training loss for the epoch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">loader</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">amp_scaler</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">gradient_clipping_threshold</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.evaluate", "modulename": "image_to_image.model_interactions.train", "qualname": "evaluate", "kind": "function", "doc": "<p>Evaluates the model on the validation set and logs results to TensorBoard or MLflow.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>model (nn.Module):\nModel to evaluate.</li>\n<li>loader (DataLoader):\nValidation data loader.</li>\n<li>criterion:\nLoss function used for evaluation.</li>\n<li>device (torch.device):\nDevice to perform inference on.</li>\n<li>writer (SummaryWriter, optional):\nTensorBoard writer for visualization.</li>\n<li>epoch (int, optional):\nCurrent epoch index for logging.</li>\n<li>save_path (str, optional):\nDirectory to save sample images.</li>\n<li>cmap (str):\nColormap for saved images.</li>\n<li>use_tqdm (bool):\nWhether to use tqdm progress bar.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>avg_loss (float): Average validation loss.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">model</span>,</span><span class=\"param\">\t<span class=\"n\">loader</span>,</span><span class=\"param\">\t<span class=\"n\">criterion</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">writer</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">save_path</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;gray&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">use_tqdm</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.save_checkpoint", "modulename": "image_to_image.model_interactions.train", "qualname": "save_checkpoint", "kind": "function", "doc": "<p>Saves a training checkpoint containing model, optimizer, and scheduler states.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>model (nn.Module):\nModel to save.</li>\n<li>optimizer:\nOptimizer or list/tuple of optimizers.</li>\n<li>scheduler:\nScheduler or list/tuple of schedulers.</li>\n<li>epoch (int):\nCurrent epoch index.</li>\n<li>path (str):\nFile path to save checkpoint to ('.pth' extension added automatically).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>None</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">optimizer</span>, </span><span class=\"param\"><span class=\"n\">scheduler</span>, </span><span class=\"param\"><span class=\"n\">epoch</span>, </span><span class=\"param\"><span class=\"n\">path</span><span class=\"o\">=</span><span class=\"s1\">&#39;ckpt.pth&#39;</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.model_interactions.train.train", "modulename": "image_to_image.model_interactions.train", "qualname": "train", "kind": "function", "doc": "<p>Main training loop for image-to-image tasks.</p>\n\n<p>Workflow:</p>\n\n<ol>\n<li>Initializes the training and validation datasets based on model type.</li>\n<li>Constructs the model and its loss functions.</li>\n<li>Configures optimizers, learning rate schedulers, and optional warm-up phases.</li>\n<li>Enables mixed precision (AMP) if selected.</li>\n<li>Sets up MLflow experiment tracking and TensorBoard visualization.</li>\n<li>Executes the epoch loop:\n<ul>\n<li>Trains the model for one epoch (<code>train_one_epoch()</code>).</li>\n<li>Optionally evaluates on the validation set.</li>\n<li>Logs metrics and learning rates.</li>\n<li>Updates the scheduler.</li>\n<li>Saves checkpoints (best or periodic).</li>\n</ul></li>\n<li>Logs the trained model and experiment results to MLflow upon completion.</li>\n</ol>\n\n<p>Parameters:</p>\n\n<ul>\n<li>args : argparse.Namespace, optional\nParsed command-line arguments containing all training configurations.\nIf None, the function will automatically call <code>parse_args()</code> to obtain them.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>None: The function performs training and logging in-place without returning values.</li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>Automatically handles model-specific configurations (e.g., Pix2Pix discriminator, ResidualDesignModel branches).</li>\n<li>Uses <code>prime.get_time()</code> to generate time-stamped run names.</li>\n<li>Supports gradient clipping and various learning rate schedulers.</li>\n</ul>\n\n<p>Logging:</p>\n\n<ul>\n<li><strong>MLflow</strong>: Stores metrics, hyperparameters, checkpoints, and final model.</li>\n<li><strong>TensorBoard</strong>: Logs training/validation losses, learning rates, and sub-loss components.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">args</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils", "modulename": "image_to_image.utils", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.utils.argument_parsing", "modulename": "image_to_image.utils.argument_parsing", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.utils.argument_parsing.get_arg_parser", "modulename": "image_to_image.utils.argument_parsing", "qualname": "get_arg_parser", "kind": "function", "doc": "<p>Creates and configures an argument parser for the Image-to-Image framework.</p>\n\n<p>This function defines and groups all command-line arguments required for \ntraining, validation, and inference of different image-to-image translation \nmodels (e.g., ResFCN, Pix2Pix, PhysicsFormer, Residual Design Model).\nIt includes parameters for model configuration, dataset handling, \noptimization, loss weighting, experiment tracking, and hardware setup.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>argparse.ArgumentParser: \nA fully configured argument parser with all options and default values.</li>\n</ul>\n\n<p>Argument Groups:\n    General:\n        --mode: Operation mode ('train', 'test', 'inference').</p>\n\n<pre><code>Training:\n    --checkpoint_save_dir: Directory to store model checkpoints.\n    --save_only_best_model: If set, saves only the best model based on validation loss.\n    --checkpoint_interval: Interval (in epochs) to save model checkpoints.\n    --validation_interval: Interval (in epochs) for validation.\n    --epochs: Number of total training epochs.\n    --batch_size: Number of samples per training batch.\n    --lr: Base learning rate.\n    --loss: Loss function to use ('l1', 'crossentropy', 'weighted_combined').\n\nWeighted Combined Loss Parameters (for primary and secondary models):\n    --wc_loss_silog_lambda, --wc_loss_weight_silog, --wc_loss_weight_grad, etc.\n    Fine-tuning coefficients for each subcomponent of the weighted combined loss.\n\nOptimization:\n    --optimizer, --optimizer_2: Optimizer types (e.g., Adam, AdamW).\n    --weight_decay: Enable or disable weight decay regularization.\n    --weight_decay_rate: Coefficient for weight decay.\n    --gradient_clipping: Enable or disable gradient clipping.\n    --gradient_clipping_threshold: Maximum gradient norm threshold.\n    --scheduler, --scheduler_2: Learning rate scheduler types ('step', 'cosine').\n    --use_warm_up: Enable warm-up phase for the optimizer.\n    --warm_up_start_lr, --warm_up_step_duration: Warm-up configuration.\n\nMixed Precision:\n    --activate_amp: Enables Automatic Mixed Precision (AMP) training.\n    --amp_scaler: Type of AMP scaler to use ('grad' or None).\n\nInference:\n    --model_params_path: Path to saved model checkpoint for inference.\n    --image_dir_path: Directory containing images for inference.\n    --output_dir: Output directory for predicted images.\n\nModel Selection:\n    --model: Choice of model architecture ('resfcn', 'pix2pix', 'residual_design_model', 'physicsformer').\n\nModel-Specific Parameters:\n    * ResFCN:\n        --resfcn_in_channels, --resfcn_hidden_channels, --resfcn_out_channels, --resfcn_num_blocks\n    * Pix2Pix:\n        --pix2pix_in_channels, --pix2pix_hidden_channels, --pix2pix_out_channels, --pix2pix_second_loss_lambda\n    * PhysicsFormer:\n        --physicsformer_in_channels, --physicsformer_out_channels, --physicsformer_img_size,\n        --physicsformer_patch_size, --physicsformer_embedded_dim, --physicsformer_num_blocks,\n        --physicsformer_heads, --physicsformer_mlp_dim, --physicsformer_dropout\n    * Residual Design Model:\n        --base_model, --complex_model, --combine_mode\n        (includes separate parameters for the second model branch, e.g., *_2)\n\nData:\n    --data_variation: Dataset variant to use (e.g., 'sound_baseline', 'sound_reflection').\n    --input_type, --output_type: Define input/output representation types.\n    --fake_rgb_output: Converts single-channel input into fake RGB format.\n    --make_14_dividable_size: Ensures image dimensions are multiples of 14.\n\nHardware:\n    --device: Compute device ('cpu' or 'cuda').\n\nExperiment Tracking:\n    --experiment_name: Group name for MLflow and TensorBoard logging.\n    --run_name: Specific run name (timestamp is prepended automatically).\n    --tensorboard_path: Directory for TensorBoard logs.\n    --save_path: Path for saving generated images during training/inference.\n    --cmap: Color map used for visualization of images.\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.argument_parsing.parse_args", "modulename": "image_to_image.utils.argument_parsing", "qualname": "parse_args", "kind": "function", "doc": "<p>Parses command-line arguments for the Image-to-Image framework.</p>\n\n<p>This function calls <code>get_arg_parser()</code> to construct the full parser, \nthen reads and returns all command-line arguments provided by the user.\nThe returned namespace contains all configuration parameters for \ntraining, validation, and inference.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>argparse.Namespace: \nParsed arguments containing configuration for model setup, \ndata handling, training hyperparameters, and logging options.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.eval_extractor", "modulename": "image_to_image.utils.eval_extractor", "kind": "module", "doc": "<p>Helper Module to change the naming of Physgen inference.</p>\n\n<p>Just gets called in the evaluation notebook.</p>\n\n<p>Its an top-level working script.</p>\n"}, {"fullname": "image_to_image.utils.package", "modulename": "image_to_image.utils.package", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.utils.package.open_notebook_folder", "modulename": "image_to_image.utils.package", "qualname": "open_notebook_folder", "kind": "function", "doc": "<p>Opens the folder containing the specified notebook in the system's file explorer.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark", "modulename": "image_to_image.utils.physgen_benchmark", "kind": "module", "doc": "<p>From: <a href=\"https://github.com/physicsgen/physicsgen/blob/main/eval_scripts/sound_metrics.py\">https://github.com/physicsgen/physicsgen/blob/main/eval_scripts/sound_metrics.py</a></p>\n\n<p>Slightly adjusted by Tobia Ippolito</p>\n\n<p>Original programmed by Martin Spitznagel</p>\n"}, {"fullname": "image_to_image.utils.physgen_benchmark.MAE", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "MAE", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">y_true</span>, </span><span class=\"param\"><span class=\"n\">y_pred</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.calc_mae", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "calc_mae", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_path</span>, </span><span class=\"param\"><span class=\"n\">pred_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.calc_mape", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "calc_mape", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_path</span>, </span><span class=\"param\"><span class=\"n\">pred_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.ray_tracing", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "ray_tracing", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image_size</span>, </span><span class=\"param\"><span class=\"n\">image_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.compute_visibility", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "compute_visibility", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">osm_path</span>, </span><span class=\"param\"><span class=\"n\">image_size</span><span class=\"o\">=</span><span class=\"mi\">256</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.masked_mae", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "masked_mae", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span>, </span><span class=\"param\"><span class=\"n\">predictions</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.masked_mape", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "masked_mape", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_labels</span>, </span><span class=\"param\"><span class=\"n\">predictions</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.calculate_sight_error", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "calculate_sight_error", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">true_path</span>, </span><span class=\"param\"><span class=\"n\">pred_path</span>, </span><span class=\"param\"><span class=\"n\">osm_path</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.utils.physgen_benchmark.evaluate_sample", "modulename": "image_to_image.utils.physgen_benchmark", "qualname": "evaluate_sample", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">true_path</span>,</span><span class=\"param\">\t<span class=\"n\">pred_path</span>,</span><span class=\"param\">\t<span class=\"n\">osm_path</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> (&lt;class &#x27;float&#x27;&gt;, &lt;class &#x27;float&#x27;&gt;, &lt;class &#x27;float&#x27;&gt;, &lt;class &#x27;float&#x27;&gt;):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data", "modulename": "image_to_image.data", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen", "modulename": "image_to_image.data.physgen", "kind": "module", "doc": "<p>PhysGen Dataset Loader</p>\n\n<p>PyTorch DataLoader.</p>\n\n<p>Also provide some functions for downloading the dataset.</p>\n\n<p>See:</p>\n\n<ul>\n<li><a href=\"https://huggingface.co/datasets/mspitzna/physicsgen\">https://huggingface.co/datasets/mspitzna/physicsgen</a></li>\n<li><a href=\"https://arxiv.org/abs/2503.05333\">https://arxiv.org/abs/2503.05333</a></li>\n<li><a href=\"https://github.com/physicsgen/physicsgen\">https://github.com/physicsgen/physicsgen</a></li>\n</ul>\n"}, {"fullname": "image_to_image.data.physgen.resize_tensor_to_divisible_by_14", "modulename": "image_to_image.data.physgen", "qualname": "resize_tensor_to_divisible_by_14", "kind": "function", "doc": "<p>Resize a tensor so that its height and width are divisible by 14.</p>\n\n<p>This function ensures the spatial dimensions (H, W) of a given tensor \nare compatible with architectures that require sizes divisible by 14 \n(e.g., ResNet, ResFCN). It resizes using bilinear interpolation.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>tensor (torch.Tensor): \nInput tensor of shape (C, H, W) or (B, C, H, W).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.Tensor: \nResized tensor with dimensions divisible by 14.</li>\n</ul>\n\n<p>Raises:</p>\n\n<ul>\n<li>ValueError: \nIf the tensor has neither 3 nor 4 dimensions.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">tensor</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset", "kind": "class", "doc": "<p>PyTorch Dataset wrapper for the PhysicsGen dataset.</p>\n\n<p>Loads the PhysGen dataset from Hugging Face and provides configurable\ninput/output modes for physics-based generative learning tasks.</p>\n\n<p>The dataset contains Open Sound Maps (OSM) and simulated soundmaps\nfor tasks involving sound propagation modeling.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.__init__", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.__init__", "kind": "function", "doc": "<p>Loads PhysGen Dataset.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>variation (str, default='sound_baseline'): \nDataset variation to load. Options include:\n{'sound_baseline', 'sound_reflection', 'sound_diffraction', 'sound_combined'}.</li>\n<li>mode (str, default='train'): \nDataset split to use. Options: {'train', 'test', 'validation'}.</li>\n<li>input_type (str, default='osm'): \nDefines the input image type:\n<ul>\n<li>'osm': open sound map input.</li>\n<li>'base_simulation': uses the baseline sound simulation as input.</li>\n</ul></li>\n<li>output_type (str, default='standard'): \nDefines the output image type:\n<ul>\n<li>'standard': full soundmap prediction.</li>\n<li>'complex_only': difference from baseline soundmap.</li>\n</ul></li>\n<li>fake_rgb_output (bool, default=False): \nIf True, replicates single-channel inputs to fake RGB (3-channel).</li>\n<li>make_14_dividable_size (bool, default=False): \nIf True, resizes tensors so that height and width are divisible by 14.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">variation</span><span class=\"o\">=</span><span class=\"s1\">&#39;sound_baseline&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">input_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;osm&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;standard&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fake_rgb_output</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">make_14_dividable_size</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.fake_rgb_output", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.fake_rgb_output", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.make_14_dividable_size", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.make_14_dividable_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.device", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.device", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.dataset", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.input_type", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.input_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.output_type", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.output_type", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.PhysGenDataset.transform", "modulename": "image_to_image.data.physgen", "qualname": "PhysGenDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.physgen.get_dataloader", "modulename": "image_to_image.data.physgen", "qualname": "get_dataloader", "kind": "function", "doc": "<p>Create a PyTorch DataLoader for the PhysGen dataset.</p>\n\n<p>This helper simplifies loading the PhysGen dataset for training,\nvalidation, or testing.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>mode (str, default='train'): \nDataset split to use ('train', 'test', 'validation').</li>\n<li>variation (str, default='sound_reflection'): \nDataset variation to load.</li>\n<li>input_type (str, default='osm'): \nDefines the input type ('osm' or 'base_simulation').</li>\n<li>output_type (str, default='complex_only'): \nDefines the output type ('standard' or 'complex_only').</li>\n<li>shuffle (bool, default=True): \nWhether to shuffle the dataset between epochs.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.utils.data.DataLoader: \nDataLoader that provides batches of PhysGen samples.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">variation</span><span class=\"o\">=</span><span class=\"s1\">&#39;sound_reflection&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">input_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;osm&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;complex_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data.physgen.get_image", "modulename": "image_to_image.data.physgen", "qualname": "get_image", "kind": "function", "doc": "<p>Retrieve one image (input and optionally output) from the PhysGen dataset.</p>\n\n<p>Provides an easy way to visualize or inspect a single PhysGen sample\nwithout manually instantiating a DataLoader.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>mode (str, default='train'): \nDataset split ('train', 'test', 'validation').\nvariation (str, default='sound_reflection'): \nDataset variation.\ninput_type (str, default='osm'): \nDefines the input type ('osm' or 'base_simulation').\noutput_type (str, default='complex_only'): \nDefines the output type ('standard' or 'complex_only').\nshuffle (bool, default=True): \nRandomly select the sample.\nreturn_output (bool, default=False): \nIf True, returns both input and target tensors.\nas_numpy_array (bool, default=True): \nIf True, converts tensors to NumPy arrays for easier visualization.</li>\n</ul>\n\n<p>Returns: </p>\n\n<ul>\n<li>numpy.ndarray or list[numpy.ndarray]: \nInput image as NumPy array, or a list [input, target] if <code>return_output</code> is True.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">variation</span><span class=\"o\">=</span><span class=\"s1\">&#39;sound_reflection&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">input_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;osm&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"s1\">&#39;complex_only&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">return_output</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">as_numpy_array</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data.physgen.save_dataset", "modulename": "image_to_image.data.physgen", "qualname": "save_dataset", "kind": "function", "doc": "<p>Save PhysGen dataset samples as images to disk.</p>\n\n<p>This function loads the specified PhysGen dataset, converts input and\ntarget tensors to images, and saves them as <code>.png</code> files for inspection,\ndebugging, or model-agnostic data use.</p>\n\n<p>Parameter: </p>\n\n<ul>\n<li>output_real_path (str): \nDirectory to save target (real) soundmaps.</li>\n<li>output_osm_path (str): \nDirectory to save input (OSM) maps.</li>\n<li>variation (str): \nDataset variation (e.g. 'sound_reflection').</li>\n<li>input_type (str): \nInput type ('osm' or 'base_simulation').</li>\n<li>output_type (str): \nOutput type ('standard' or 'complex_only').</li>\n<li>data_mode (str): \nDataset split ('train', 'test', 'validation').</li>\n<li>info_print (bool, default=False): \nIf True, prints detailed information for each saved sample.</li>\n<li>progress_print (bool, default=True): \nIf True, shows progress updates in the console.</li>\n</ul>\n\n<p>Raises:</p>\n\n<ul>\n<li>ValueError: \nIf image data falls outside the valid range [0, 255].</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">output_real_path</span>,</span><span class=\"param\">\t<span class=\"n\">output_osm_path</span>,</span><span class=\"param\">\t<span class=\"n\">variation</span>,</span><span class=\"param\">\t<span class=\"n\">input_type</span>,</span><span class=\"param\">\t<span class=\"n\">output_type</span>,</span><span class=\"param\">\t<span class=\"n\">data_mode</span>,</span><span class=\"param\">\t<span class=\"n\">info_print</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">progress_print</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data.residual_physgen", "modulename": "image_to_image.data.residual_physgen", "kind": "module", "doc": "<p>A PhysGen Dataset Wrapper to get base-propagation and \ncomplex-propagation in one dataloader.</p>\n\n<p>See:</p>\n\n<ul>\n<li><a href=\"https://huggingface.co/datasets/mspitzna/physicsgen\">https://huggingface.co/datasets/mspitzna/physicsgen</a></li>\n<li><a href=\"https://arxiv.org/abs/2503.05333\">https://arxiv.org/abs/2503.05333</a></li>\n<li><a href=\"https://github.com/physicsgen/physicsgen\">https://github.com/physicsgen/physicsgen</a></li>\n</ul>\n"}, {"fullname": "image_to_image.data.residual_physgen.to_device", "modulename": "image_to_image.data.residual_physgen", "qualname": "to_device", "kind": "function", "doc": "<p>Move dataset tensors to the appropriate device (CPU or GPU).</p>\n\n<p>This helper function expects a dataset item formatted as \n[input_tensor, target_tensor, index]. It automatically moves \nall tensor elements to the available device.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>dataset (list): \nA list of three elements: \n[input_tensor (torch.Tensor), target_tensor (torch.Tensor), index (int)].</li>\n</ul>\n\n<p>Returns: </p>\n\n<ul>\n<li>list: \nA list [input_tensor_on_device, target_tensor_on_device, index].</li>\n</ul>\n\n<p>Raises:</p>\n\n<ul>\n<li>ValueError: \nIf the provided dataset item does not have exactly 3 elements.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">dataset</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset", "kind": "class", "doc": "<p>Dataset wrapper combining multiple PhysGen dataset variations \nfor residual learning experiments.</p>\n\n<p>This dataset constructs three related PhysGen datasets:</p>\n\n<ol>\n<li>Baseline dataset (sound_baseline \u2192 'standard' output)</li>\n<li>Complex dataset (user-selected variation \u2192 'complex_only' output)</li>\n<li>Fusion dataset (user-selected variation \u2192 'standard' output)</li>\n</ol>\n\n<p>It is designed for residual or multi-source learning setups \nwhere the model uses both baseline and complex physics simulations.</p>\n", "bases": "typing.Generic[+_T_co]"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.__init__", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.__init__", "kind": "function", "doc": "<p>Initialize the PhysGenResidualDataset with multiple data sources.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>variation (str, default='sound_baseline'): \nSpecifies which physics variation to use for the complex and fusion datasets.\nCommon options: {'sound_reflection', 'sound_diffraction', 'sound_combined'}.</li>\n<li>mode (str, default='train'): \nSpecifies dataset mode. Options: {'train', 'validation'}.</li>\n<li>fake_rgb_output (bool, default=False): \nIf True, single-channel inputs are expanded to fake RGB format.</li>\n<li>make_14_dividable_size (bool, default=False): \nIf True, ensures images are resized so that their height and width are divisible by 14.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">variation</span><span class=\"o\">=</span><span class=\"s1\">&#39;sound_baseline&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;train&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">fake_rgb_output</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">make_14_dividable_size</span><span class=\"o\">=</span><span class=\"kc\">False</span></span>)</span>"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.train_dataset_base", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.train_dataset_base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.val_dataset_base", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.val_dataset_base", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.train_dataset_complex", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.train_dataset_complex", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.val_dataset_complex", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.val_dataset_complex", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.train_dataset_fusion", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.train_dataset_fusion", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.val_dataset_fusion", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.val_dataset_fusion", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.data.residual_physgen.PhysGenResidualDataset.datasets", "modulename": "image_to_image.data.residual_physgen", "qualname": "PhysGenResidualDataset.datasets", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses", "modulename": "image_to_image.losses", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "kind": "module", "doc": "<p>Module to define a Weighted Combine Loss.</p>\n\n<p>Functions:</p>\n\n<ul>\n<li>calc_weight_map</li>\n</ul>\n\n<p>Classes:</p>\n\n<ul>\n<li>WeightedCombinedLoss</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss", "kind": "class", "doc": "<p>Computes a weighted combination of multiple loss functions for image-to-image tasks.</p>\n\n<p>Supported losses:\n    - SILog loss\n    - Gradient L1 loss\n    - SSIM loss\n    - Edge-aware loss\n    - L1 loss\n    - Variance loss\n    - Range loss\n    - Blur loss</p>\n\n<p>The losses can be weighted individually, and average losses are tracked across steps.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.__init__", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.__init__", "kind": "function", "doc": "<p>Initializes the WeightedCombinedLoss with optional weights for each component.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>silog_lambda (float): \nSILog lambda parameter.</li>\n<li>weight_silog (float): \nWeight for SILog loss.</li>\n<li>weight_grad (float): \nWeight for gradient L1 loss.</li>\n<li>weight_ssim (float): \nWeight for SSIM loss.</li>\n<li>weight_edge_aware (float): \nWeight for edge-aware loss.</li>\n<li>weight_l1 (float): \nWeight for L1 loss.</li>\n<li>weight_var (float): \nWeight for variance loss.</li>\n<li>weight_range (float): \nWeight for range loss.</li>\n<li>weight_blur (float): \nWeight for blur loss.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">silog_lambda</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">weight_silog</span><span class=\"o\">=</span><span class=\"mf\">0.5</span>,</span><span class=\"param\">\t<span class=\"n\">weight_grad</span><span class=\"o\">=</span><span class=\"mf\">10.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_ssim</span><span class=\"o\">=</span><span class=\"mf\">5.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_edge_aware</span><span class=\"o\">=</span><span class=\"mf\">10.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_l1</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_var</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_range</span><span class=\"o\">=</span><span class=\"mf\">1.0</span>,</span><span class=\"param\">\t<span class=\"n\">weight_blur</span><span class=\"o\">=</span><span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.silog_lambda", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.silog_lambda", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_silog", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_silog", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_grad", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_grad", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_ssim", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_ssim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_edge_aware", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_edge_aware", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_l1", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_l1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_var", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_var", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_range", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_range", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.weight_blur", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.weight_blur", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_silog", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_silog", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_grad", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_grad", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_ssim", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_ssim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_l1", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_l1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_edge_aware", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_edge_aware", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_var", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_var", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_range", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_range", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.avg_loss_blur", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.avg_loss_blur", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.steps", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.steps", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.ssim_module", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.ssim_module", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.silog_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.silog_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.gradient_l1_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.gradient_l1_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.ssim_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.ssim_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.edge_aware_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.edge_aware_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.l1_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.l1_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.variance_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.variance_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.range_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.range_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.blur_loss", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.blur_loss", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.forward", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.forward", "kind": "function", "doc": "<p>Computes the weighted combined loss between prediction and target.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>pred (torch.Tensor): \nPredicted output tensor.</li>\n<li>target (torch.Tensor): \nGround truth tensor.</li>\n<li>weight_map (torch.Tensor or None): \nOptional pixel-wise weighting map.</li>\n<li>should_calc_weight_map (bool): \nIf True and weight_map is None, calculates a weight map from target.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.Tensor: Weighted sum of all losses.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">pred</span>, </span><span class=\"param\"><span class=\"n\">target</span>, </span><span class=\"param\"><span class=\"n\">weight_map</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">should_calc_weight_map</span><span class=\"o\">=</span><span class=\"kc\">False</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.step", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.step", "kind": "function", "doc": "<p>Resets the running averages of all tracked losses.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">epoch</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.get_avg_losses", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.get_avg_losses", "kind": "function", "doc": "<p>Returns the running average of all individual losses.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>tuple: (avg_loss_silog, avg_loss_grad, avg_loss_ssim, avg_loss_l1,\navg_loss_edge_aware, avg_loss_var, avg_loss_range, avg_loss_blur)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.WeightedCombinedLoss.get_dict", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "WeightedCombinedLoss.get_dict", "kind": "function", "doc": "<p>Returns a dictionary of average losses and their corresponding weights.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>dict: All loss components with their weights.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.losses.weighted_combined_loss.calc_weight_map", "modulename": "image_to_image.losses.weighted_combined_loss", "qualname": "calc_weight_map", "kind": "function", "doc": "<p>Calculates a per-pixel weighting map for a target tensor based on unique value frequencies.</p>\n\n<p>Less frequent values are given higher weights to emphasize their contribution in loss computations.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>target (torch.Tensor): \nGround truth tensor.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.Tensor: Weight map tensor of the same shape as target.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">target</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models", "modulename": "image_to_image.models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix", "modulename": "image_to_image.models.pix2pix", "kind": "module", "doc": "<p>Module to define a Pix2Pix Model. \nA UNet CNN Generator combined with a small generative loss.</p>\n\n<p>Functions:</p>\n\n<ul>\n<li>unet_down_block</li>\n<li>unet_up_block</li>\n</ul>\n\n<p>Classes:</p>\n\n<ul>\n<li>MMC</li>\n<li>UNetGenerator</li>\n<li>Discriminator</li>\n<li>Pix2Pix</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.models.pix2pix.MMC", "modulename": "image_to_image.models.pix2pix", "qualname": "MMC", "kind": "class", "doc": "<p>Min-Max Clamping Module.</p>\n\n<p>Clamps input tensor values between a specified minimum and maximum.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>min (float): \nMinimum allowed value (default=0.0).</li>\n<li>max (float): \nMaximum allowed value (default=1.0).</li>\n</ul>\n\n<p>Usage:</p>\n\n<ul>\n<li>Can be used at the output layer of a generator to ensure predictions remain in a valid range.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.pix2pix.MMC.__init__", "modulename": "image_to_image.models.pix2pix", "qualname": "MMC.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"nb\">min</span><span class=\"o\">=</span><span class=\"mf\">0.0</span>, </span><span class=\"param\"><span class=\"nb\">max</span><span class=\"o\">=</span><span class=\"mf\">1.0</span></span>)</span>"}, {"fullname": "image_to_image.models.pix2pix.MMC.min", "modulename": "image_to_image.models.pix2pix", "qualname": "MMC.min", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.MMC.max", "modulename": "image_to_image.models.pix2pix", "qualname": "MMC.max", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.MMC.forward", "modulename": "image_to_image.models.pix2pix", "qualname": "MMC.forward", "kind": "function", "doc": "<p>Forward pass.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput tensor.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Clamped tensor with values between <code>min</code> and <code>max</code>.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.unet_down_block", "modulename": "image_to_image.models.pix2pix", "qualname": "unet_down_block", "kind": "function", "doc": "<p>Creates a U-Net downsampling block.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>in_channels (int): \nNumber of input channels.</li>\n<li>out_channels (int): \nNumber of output channels.</li>\n<li>normalize (bool): \nWhether to apply instance normalization.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>nn.Sequential: Convolutional downsampling block with LeakyReLU activation.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">normalize</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.unet_up_block", "modulename": "image_to_image.models.pix2pix", "qualname": "unet_up_block", "kind": "function", "doc": "<p>Creates a U-Net upsampling block.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>in_channels (int): \nNumber of input channels.</li>\n<li>out_channels (int): \nNumber of output channels.</li>\n<li>dropout (float): \nDropout probability (default=0).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>nn.Sequential: Transposed convolutional block with ReLU activation and optional dropout.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">in_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">out_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.0</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator", "kind": "class", "doc": "<p>U-Net Generator for image-to-image translation.</p>\n\n<p>Architecture:</p>\n\n<ul>\n<li>8 downsampling blocks (encoder)</li>\n<li>8 upsampling blocks (decoder) with skip connections</li>\n<li>Sigmoid activation at output for [0,1] pixel normalization</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input image channels.</li>\n<li>output_channels (int): \nNumber of output image channels.</li>\n<li>hidden_channels (int): \nBase hidden channels for first layer.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.__init__", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span></span>)</span>"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down1", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down2", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down3", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down4", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down5", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down5", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down6", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down6", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down7", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down7", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.down8", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.down8", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up1", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up2", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up3", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up4", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up4", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up5", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up5", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up6", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up6", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up7", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up7", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.up8", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.up8", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.UNetGenerator.forward", "modulename": "image_to_image.models.pix2pix", "qualname": "UNetGenerator.forward", "kind": "function", "doc": "<p>Forward pass of the U-Net generator.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput image tensor (batch_size, input_channels, H, W).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Generated output tensor (batch_size, output_channels, H, W).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Discriminator", "modulename": "image_to_image.models.pix2pix", "qualname": "Discriminator", "kind": "class", "doc": "<p>PatchGAN Discriminator for Pix2Pix GAN.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input channels (typically input + target channels concatenated).</li>\n<li>hidden_channels (int): \nBase hidden channels for first layer.</li>\n</ul>\n\n<p>Architecture:</p>\n\n<ul>\n<li>5 convolutional blocks with LeakyReLU and batch normalization.</li>\n<li>Outputs a 2D patch map of predictions.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.pix2pix.Discriminator.__init__", "modulename": "image_to_image.models.pix2pix", "qualname": "Discriminator.__init__", "kind": "function", "doc": "<p>Initializes a PatchGAN discriminator.</p>\n\n<p>The discriminator evaluates input-target image pairs to determine\nif they are real or generated (fake). It progressively downsamples\nthe spatial dimensions while increasing the number of feature channels.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input channels, typically input + target concatenated (default=6).</li>\n<li>hidden_channels (int): \nNumber of channels in the first convolutional layer; doubled in subsequent layers (default=64).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">6</span>, </span><span class=\"param\"><span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span></span>)</span>"}, {"fullname": "image_to_image.models.pix2pix.Discriminator.model", "modulename": "image_to_image.models.pix2pix", "qualname": "Discriminator.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Discriminator.forward", "modulename": "image_to_image.models.pix2pix", "qualname": "Discriminator.forward", "kind": "function", "doc": "<p>Forward pass of the discriminator.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput image tensor.</li>\n<li>y (torch.tensor): \nTarget or generated image tensor.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: PatchGAN output tensor predicting real/fake for each patch.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix", "kind": "class", "doc": "<p>Pix2Pix GAN for image-to-image translation.</p>\n\n<p>Components:</p>\n\n<ul>\n<li>Generator: U-Net generator producing synthetic images.</li>\n<li>Discriminator: PatchGAN discriminator evaluating real vs fake images.</li>\n<li>Adversarial loss: Binary cross-entropy.</li>\n<li>Optional second loss for pixel-wise supervision.</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input channels.</li>\n<li>output_channels (int): \nNumber of output channels.</li>\n<li>hidden_channels (int): \nBase hidden channels for both generator and discriminator.</li>\n<li>second_loss (nn.Module): \nOptional secondary loss (default: L1Loss).</li>\n<li>lambda_second (float): \nWeight for secondary loss in generator optimization.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.__init__", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.__init__", "kind": "function", "doc": "<p>Initializes the Pix2Pix GAN model.</p>\n\n<p>Components:</p>\n\n<ul>\n<li>Generator: U-Net architecture for producing synthetic images.</li>\n<li>Discriminator: PatchGAN for evaluating real vs. fake images.</li>\n<li>Adversarial loss: Binary cross-entropy to train the generator to fool the discriminator.</li>\n<li>Optional secondary loss: Pixel-wise supervision (default: L1Loss).</li>\n</ul>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of channels in the input images (default=1).</li>\n<li>output_channels (int): \nNumber of channels in the output images (default=1).</li>\n<li>hidden_channels (int): \nBase number of hidden channels in the generator and discriminator (default=64).</li>\n<li>second_loss (nn.Module): \nOptional secondary loss for the generator (default: nn.L1Loss()).</li>\n<li>lambda_second (float): \nWeight applied to the secondary loss in generator optimization (default=100).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">second_loss</span><span class=\"o\">=</span><span class=\"n\">L1Loss</span><span class=\"p\">()</span>,</span><span class=\"param\">\t<span class=\"n\">lambda_second</span><span class=\"o\">=</span><span class=\"mi\">100</span></span>)</span>"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.input_channels", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.output_channels", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.output_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.generator", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.generator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.discriminator", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.discriminator", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.adversarial_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.adversarial_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.second_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.second_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.lambda_second", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.lambda_second", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.last_generator_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.last_generator_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.last_generator_adversarial_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.last_generator_adversarial_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.last_generator_second_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.last_generator_second_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.last_discriminator_loss", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.last_discriminator_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.get_input_channels", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.get_input_channels", "kind": "function", "doc": "<p>Returns the number of input channels used by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of input channels expected by the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.get_output_channels", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.get_output_channels", "kind": "function", "doc": "<p>Returns the number of output channels produced by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of output channels the model generates</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.get_dict", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.get_dict", "kind": "function", "doc": "<p>Returns a dictionary with the most recent loss values.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>dict: Loss components (base, complex).</li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>Useful for logging or monitoring training progress.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.forward", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.forward", "kind": "function", "doc": "<p>Forward pass through the generator.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput tensor.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Generated output image.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.generator_step", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.generator_step", "kind": "function", "doc": "<p>Performs a single optimization step for the generator.</p>\n\n<p>This includes:</p>\n\n<ul>\n<li>Forward pass through the generator and discriminator.</li>\n<li>Computing adversarial loss (generator tries to fool the discriminator).</li>\n<li>Computing optional secondary loss (e.g., L1 or MSE).</li>\n<li>Backpropagation and optimizer step, optionally with AMP and gradient clipping.</li>\n</ul>\n\n<p>Parameters:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput tensor for the generator (e.g., source image).</li>\n<li>y (torch.tensor): \nTarget tensor for supervised secondary loss.</li>\n<li>optimizer (torch.optim.Optimizer): \nOptimizer for the generator parameters.</li>\n<li>amp_scaler (torch.cuda.amp.GradScaler or None): \nAutomatic mixed precision scaler.</li>\n<li>device (torch.device): \nDevice for AMP autocast.</li>\n<li>gradient_clipping_threshold (float or None): \nMax norm for gradient clipping; if None, no clipping.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>tuple(torch.tensor, torch.tensor, torch.tensor):\n<ul>\n<li>Total generator loss (adversarial + secondary).</li>\n<li>Adversarial loss component.</li>\n<li>Secondary loss component (weighted by <code>lambda_second</code>).</li>\n</ul></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>If AMP is enabled, gradients are scaled and unscaled appropriately.</li>\n<li><code>last_generator_loss</code>, <code>last_generator_adversarial_loss</code>, and <code>last_generator_second_loss</code> are updated.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">y</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">amp_scaler</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">gradient_clipping_threshold</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.pix2pix.Pix2Pix.discriminator_step", "modulename": "image_to_image.models.pix2pix", "qualname": "Pix2Pix.discriminator_step", "kind": "function", "doc": "<p>Performs a single optimization step for the discriminator.</p>\n\n<p>This includes:</p>\n\n<ul>\n<li>Forward pass through the discriminator for both real and fake samples.</li>\n<li>Computing adversarial loss (binary cross-entropy) for real vs fake patches.</li>\n<li>Backpropagation and optimizer step, optionally with AMP and gradient clipping.</li>\n</ul>\n\n<p>Parameters:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput tensor (e.g., source image).</li>\n<li>y (torch.tensor): \nTarget tensor (real image) for the discriminator.</li>\n<li>optimizer (torch.optim.Optimizer): \nOptimizer for the discriminator parameters.</li>\n<li>amp_scaler (torch.cuda.amp.GradScaler or None): \nAutomatic mixed precision scaler.</li>\n<li>device (torch.device): \nDevice for AMP autocast.</li>\n<li>gradient_clipping_threshold (float or None): \nMax norm for gradient clipping; if None, no clipping.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>tuple(torch.tensor, torch.tensor, torch.tensor):\n<ul>\n<li>Total discriminator loss (mean of real and fake losses).</li>\n<li>Loss for real samples.</li>\n<li>Loss for fake samples.</li>\n</ul></li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>Fake images are detached from the generator to prevent updating its weights.</li>\n<li><code>last_discriminator_loss</code> is updated.</li>\n<li>Supports AMP and optional gradient clipping for stability.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">x</span>,</span><span class=\"param\">\t<span class=\"n\">y</span>,</span><span class=\"param\">\t<span class=\"n\">optimizer</span>,</span><span class=\"param\">\t<span class=\"n\">amp_scaler</span>,</span><span class=\"param\">\t<span class=\"n\">device</span>,</span><span class=\"param\">\t<span class=\"n\">gradient_clipping_threshold</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.resfcn", "modulename": "image_to_image.models.resfcn", "kind": "module", "doc": "<p>Module to define a simple CNN Model for image to image. </p>\n\n<p>Classes:</p>\n\n<ul>\n<li>ResidualBlock</li>\n<li>ResFCN</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock", "kind": "class", "doc": "<p>A simple residual block using fully convolutional layers.</p>\n\n<p>The block applies two convolutional layers with an activation in between\nand adds the input to the output (skip connection) to preserve features.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock.__init__", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock.__init__", "kind": "function", "doc": "<p>Initializes the ResidualBlock.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>channels (int): \nNumber of input and output channels.</li>\n<li>kernel_size (int): \nKernel size for the convolutional layers (default=3).</li>\n<li>padding (int): \nPadding for the convolutional layers to maintain spatial dimensions (default=1).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">channels</span>, </span><span class=\"param\"><span class=\"n\">kernel_size</span><span class=\"o\">=</span><span class=\"mi\">3</span>, </span><span class=\"param\"><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"mi\">1</span></span>)</span>"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock.pre_fcn", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock.pre_fcn", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock.activation", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock.activation", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock.post_fcn", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock.post_fcn", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResidualBlock.forward", "modulename": "image_to_image.models.resfcn", "qualname": "ResidualBlock.forward", "kind": "function", "doc": "<p>Forward pass through the ResidualBlock.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput tensor of shape [B, C, H, W].</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Output tensor after residual connection, same shape as input.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.resfcn.ResFCN", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN", "kind": "class", "doc": "<p>A simple image-to-image model using Fully Convolutional Networks (FCN) with residual connections.</p>\n\n<p>Architecture:</p>\n\n<ul>\n<li>Initial convolution layer to increase channel depth.</li>\n<li>Sequence of ResidualBlocks with varying kernel sizes.</li>\n<li>Final convolution layer to project back to output channels.</li>\n<li>Output is clamped between 0.0 and 1.0.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.resfcn.ResFCN.__init__", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.__init__", "kind": "function", "doc": "<p>Initializes the ResFCN model.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of channels in the input image (default=1).</li>\n<li>hidden_channels (int): \nNumber of channels in hidden layers / residual blocks (default=64).</li>\n<li>output_channels (int): \nNumber of channels in the output image (default=1).</li>\n<li>num_blocks (int): \nNumber of residual blocks to apply (default=64).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">num_blocks</span><span class=\"o\">=</span><span class=\"mi\">64</span></span>)</span>"}, {"fullname": "image_to_image.models.resfcn.ResFCN.input_channels", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResFCN.output_channels", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.output_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResFCN.pre_layer", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.pre_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResFCN.residual_fcn_layers", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.residual_fcn_layers", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResFCN.post_layer", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.post_layer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.resfcn.ResFCN.get_input_channels", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.get_input_channels", "kind": "function", "doc": "<p>Returns the number of input channels used by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of input channels expected by the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.resfcn.ResFCN.get_output_channels", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.get_output_channels", "kind": "function", "doc": "<p>Returns the number of output channels produced by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of output channels the model generates</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.resfcn.ResFCN.forward", "modulename": "image_to_image.models.resfcn", "qualname": "ResFCN.forward", "kind": "function", "doc": "<p>Forward pass through the ResFCN model.</p>\n\n<p>Parameters:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput image tensor of shape [B, C, H, W].</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Output image tensor, same spatial size as input, values clamped between 0.0 and 1.0.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model", "modulename": "image_to_image.models.residual_design_model", "kind": "module", "doc": "<p>Module to define a Residual-Design Model.<br>\nA model which consists of 2 models.</p>\n\n<p>The data is (should) be splitted up in 2 parts (sub-problems) \nfor example in Physgen Dataset, the base-propagation and \nthe complex-propagation.</p>\n\n<p>Classes:</p>\n\n<ul>\n<li>CombineNet</li>\n<li>ResidualDesignModel</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet", "kind": "class", "doc": "<p>Helper network for combining two images. \nThe two outputs of the submodels of the ResidualDesignModel. </p>\n\n<p>The CombineNet is a lightweight convolutional neural network that takes \ntwo input tensors, merges them channel-wise, and learns to predict a \ncombined output representation. It can be used for post-processing, \nfusion of multiple model outputs, or blending of different feature spaces.</p>\n\n<p>Architecture Overview:</p>\n\n<ul>\n<li>3 convolutional layers with batch normalization and GELU activation.</li>\n<li>Final sigmoid activation to normalize outputs between [0, 1].</li>\n<li>Optimized using L1 loss (Mean Absolute Error).</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.__init__", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.__init__", "kind": "function", "doc": "<p>Init of the CombineNet model.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of channels for each of the two input tensors \n(e.g., 1 for grayscale, 3 for RGB).</li>\n<li>output_channels (int): \nNumber of output channels of the combined result.</li>\n<li>hidden_channels (int): \nNumber of feature channels in the hidden layers (internal representation).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">32</span></span>)</span>"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.model", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.last_loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.last_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.optimizer", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.forward", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.forward", "kind": "function", "doc": "<p>Forward pass of the CombineNet.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nFirst input tensor of shape (batch_size, input_channels, height, width).</li>\n<li>y (torch.tensor): \nSecond input tensor of shape (batch_size, input_channels, height, width).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nCombined output tensor of shape (batch_size, output_channels, height, width).</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.CombineNet.backward", "modulename": "image_to_image.models.residual_design_model", "qualname": "CombineNet.backward", "kind": "function", "doc": "<p>Backward pass (training step) for the CombineNet.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>y_base (torch.tensor): \nFirst input tensor (e.g., base model output).</li>\n<li>y_complex (torch.tensor): \nSecond input tensor (e.g., complex or refined prediction).</li>\n<li>y (torch.tensor): \nGround truth tensor (target output for supervision).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>float: \nThe scalar loss value (L1 loss) from the current optimization step.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">y_base</span>, </span><span class=\"param\"><span class=\"n\">y_complex</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel", "kind": "class", "doc": "<p>Residual Design Model for combining predictions from a base and a complex model.</p>\n\n<p>The ResidualDesignModel enables two modes of combination:</p>\n\n<ol>\n<li><strong>Mathematical Residual (<code>math</code>)</strong>:\n<ul>\n<li>Computes a weighted sum of the base and complex model outputs.</li>\n<li>The weight <code>alpha</code> is learnable and optimized via L1 loss.</li>\n</ul></li>\n<li><strong>Neural Network Fusion (<code>nn</code>)</strong>:\n<ul>\n<li>Uses a small CNN (<code>CombineNet</code>) to learn a nonlinear combination of the outputs.</li>\n</ul></li>\n</ol>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.__init__", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.__init__", "kind": "function", "doc": "<p>Init of the ResidualDesignModel.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>base_model (nn.Module): Pretrained or instantiated base model.</li>\n<li>complex_model (nn.Module): Pretrained or instantiated complex model.</li>\n<li>combine_mode (str, default='math'): Mode for combining outputs. Options:\n<ul>\n<li>'math': Weighted residual combination with learnable alpha.</li>\n<li>'nn': Nonlinear fusion using a small CombineNet.</li>\n</ul></li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">base_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">complex_model</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">combine_mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;math&#39;</span></span>)</span>"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.base_model", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.base_model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.complex_model", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.complex_model", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.combine_mode", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.combine_mode", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.input_channels", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.output_channels", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.output_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.combine_net", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.combine_net", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.alpha", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.alpha", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.alpha_optimizer", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.alpha_optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.alpha_criterion", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.alpha_criterion", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.last_base_loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.last_base_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.last_complex_loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.last_complex_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.last_combined_loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.last_combined_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.last_combined_math_loss", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.last_combined_math_loss", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.get_input_channels", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.get_input_channels", "kind": "function", "doc": "<p>Returns a tuple with the input channels of the base and complex models.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>tuple: (base_model_input_channels, complex_model_input_channels)</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.get_output_channels", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.get_output_channels", "kind": "function", "doc": "<p>Returns the number of output channels for the combined prediction.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: Minimum of base_model and complex_model output channels.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.forward", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.forward", "kind": "function", "doc": "<p>Forward pass of the ResidualDesignModel.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x_base (torch.tensor): Input tensor for the base model.</li>\n<li>x_complex (torch.tensor): Input tensor for the complex model.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: Combined prediction, either via weighted residual or CombineNet.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x_base</span>, </span><span class=\"param\"><span class=\"n\">x_complex</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.backward", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.backward", "kind": "function", "doc": "<p>Backward pass to optimize the alpha parameter for mathematical residual combination.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>y_base (torch.tensor): \nOutput of the base model.</li>\n<li>y_complex (torch.tensor): \nOutput of the complex model.</li>\n<li>y (torch.tensor): \nGround truth tensor.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">y_base</span>, </span><span class=\"param\"><span class=\"n\">y_complex</span>, </span><span class=\"param\"><span class=\"n\">y</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.residual_design_model.ResidualDesignModel.get_dict", "modulename": "image_to_image.models.residual_design_model", "qualname": "ResidualDesignModel.get_dict", "kind": "function", "doc": "<p>Returns a dictionary with the most recent loss values.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>dict: Loss components (base, complex).</li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>Useful for logging or monitoring training progress.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer", "modulename": "image_to_image.models.transformer", "kind": "module", "doc": "<p>Module to define basic Transformer parts.<br>\nAlso defines a Transformer model for image-to-image tasks, the PhysicsFormer.</p>\n\n<p>Classes:</p>\n\n<ul>\n<li>PatchEmbedding</li>\n<li>PositionalEncoding</li>\n<li>Attention</li>\n<li>TransformerEncoderBlock</li>\n<li>PhysicsFormer</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding", "kind": "class", "doc": "<p>Converts an Image into Patches ('Tokens' in NLP).</p>\n\n<p>Image size must be: H x W x C</p>\n\n<p>Patch size must be: P x P</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.__init__", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.__init__", "kind": "function", "doc": "<p>Init of Patching.</p>\n\n<p>Each filter creates one new value for each patch \nand this with embedded_dim-filters. <br>\nSo one patch is projected into a 'embedded_dim'-vector. <br>\nFor example the value at [0, 0] on each channel in the embedded image is together \nthe embedded vector of the first patch.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>img_size (int): \nSize (width or height) of your image. Your image must have the same width and height.</li>\n<li>patch_size (int, default=16): \nSize (width or height) of one patch.</li>\n<li>input_channels (int): \nNumber of Input Channels to be expected.</li>\n<li>embedded_dim (int): \nOutput channels / channels of the embedding.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">img_size</span>, </span><span class=\"param\"><span class=\"n\">patch_size</span><span class=\"o\">=</span><span class=\"mi\">16</span>, </span><span class=\"param\"><span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>, </span><span class=\"param\"><span class=\"n\">embedded_dim</span><span class=\"o\">=</span><span class=\"mi\">768</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.img_size", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.img_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.patch_size", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.patch_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.in_channels", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.in_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.embedded_dim", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.embedded_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.projection", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.projection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PatchEmbedding.forward", "modulename": "image_to_image.models.transformer", "qualname": "PatchEmbedding.forward", "kind": "function", "doc": "<p>Forward pass of patching.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput Image(s).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>tuple(torch.tensor, tuple(int, int)): \nThe Embedded image with the height and width.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.PositionalEncoding", "modulename": "image_to_image.models.transformer", "qualname": "PositionalEncoding", "kind": "class", "doc": "<p>Add learnable parameters which adds positional information of the patches \nthe position of a patch is important, because a picture makes only sense \nif the other of sub pictures (patches) is right.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.PositionalEncoding.__init__", "modulename": "image_to_image.models.transformer", "qualname": "PositionalEncoding.__init__", "kind": "function", "doc": "<p>Init of Positonal Encoding.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>num_patches (int): \nAmount of Patches ('Tokens').</li>\n<li>embedded_dim (int, default=768): \nGet amount of the embedding channels.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">num_patches</span>, </span><span class=\"param\"><span class=\"n\">embedded_dim</span><span class=\"o\">=</span><span class=\"mi\">768</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.PositionalEncoding.positional_embedding", "modulename": "image_to_image.models.transformer", "qualname": "PositionalEncoding.positional_embedding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PositionalEncoding.forward", "modulename": "image_to_image.models.transformer", "qualname": "PositionalEncoding.forward", "kind": "function", "doc": "<p>Forward pass of positional information adding.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nPatch Embedded Image(s).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nThe Embedded image(s) with positional encoding added.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.Attention", "modulename": "image_to_image.models.transformer", "qualname": "Attention", "kind": "class", "doc": "<p>Basic element of Transformer are the attention-layer. <br>\nAttention layer computes relations to all patches. <br>\nThis is done by calculating the similarity between 2 learnable vectors ! and K.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.Attention.__init__", "modulename": "image_to_image.models.transformer", "qualname": "Attention.__init__", "kind": "function", "doc": "<p>Init of Attention Layer.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>embedded_dim (int): \nPatch Embedding Channels.</li>\n<li>num_heads (int): \nNumber of parallel attention computations.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embedded_dim</span>, </span><span class=\"param\"><span class=\"n\">num_heads</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.Attention.num_heads", "modulename": "image_to_image.models.transformer", "qualname": "Attention.num_heads", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.Attention.head_dim", "modulename": "image_to_image.models.transformer", "qualname": "Attention.head_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.Attention.scale", "modulename": "image_to_image.models.transformer", "qualname": "Attention.scale", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.Attention.qkv", "modulename": "image_to_image.models.transformer", "qualname": "Attention.qkv", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.Attention.fc", "modulename": "image_to_image.models.transformer", "qualname": "Attention.fc", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.Attention.forward", "modulename": "image_to_image.models.transformer", "qualname": "Attention.forward", "kind": "function", "doc": "<p>Forward pass of Attention Layer.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nPatch Embedded Image(s) with positional encoding added.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nThe attention cores passed through fully connected layer.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock", "kind": "class", "doc": "<p>A Transformer Encoder Block consists of self-attention layer \nfollowed by a feedforward layer (mlp = multilayerperceptron) </p>\n\n<ul>\n<li>(layer) normalization \nand with residual connections / skip connections.</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.__init__", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.__init__", "kind": "function", "doc": "<p>Init of a Transformer Block.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>embedded_dim (int): \nPatch Embedding Channels.</li>\n<li>num_heads (int): \nNumber of parallel attention computations.</li>\n<li>mlp_dim (int): \nHidden/Feature dimension of the multi layer perceptron layer.</li>\n<li>dropout (float): \nPropability of droput regulization.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">embedded_dim</span>, </span><span class=\"param\"><span class=\"n\">num_heads</span>, </span><span class=\"param\"><span class=\"n\">mlp_dim</span>, </span><span class=\"param\"><span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.norm_1", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.norm_1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.attention", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.attention", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.norm_2", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.norm_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.mlp", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.mlp", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.TransformerEncoderBlock.forward", "modulename": "image_to_image.models.transformer", "qualname": "TransformerEncoderBlock.forward", "kind": "function", "doc": "<p>Forward pass of Transformer Block.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nPatch Embedded Image(s) with positional encoding added.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nThe attention cores passed through fully connected layer and a multilayer perceptron with layer normalization.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.CNNRefinement", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement", "kind": "class", "doc": "<p>Refinement Network to remove transformer artefacts.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.__init__", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.__init__", "kind": "function", "doc": "<p>Init of a CNN Refinement network.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input channels.</li>\n<li>hidden_channels (int): \nNumber of hidden/feature channels.</li>\n<li>output_channels (int): \nNumber of output channels.</li>\n<li>skip_connection (bool): \nShould a skip connection be used? Means if a second input (the original image) should be added to the output. \nThat changes the CNN network to learning a correction which will be applied to the original image.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_channels</span><span class=\"o\">=</span><span class=\"mi\">64</span>,</span><span class=\"param\">\t<span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">skip_connection</span><span class=\"o\">=</span><span class=\"kc\">True</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.conv_1", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.conv_1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.activation_1", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.activation_1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.conv_2", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.conv_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.activation_2", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.activation_2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.conv_3", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.conv_3", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.skip_connection", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.skip_connection", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.CNNRefinement.forward", "modulename": "image_to_image.models.transformer", "qualname": "CNNRefinement.forward", "kind": "function", "doc": "<p>Forward pass of CNN Refinement network.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nPatch Embedded Image(s) with positional encoding added.</li>\n<li>original (torch.tensor or None, default=None): \nOriginal image which will be added at the end if skip_connection is set to true.</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nThe refined image.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span>, </span><span class=\"param\"><span class=\"n\">original</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.PhysicFormer", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer", "kind": "class", "doc": "<p>Image-to-Image Transformer.</p>\n\n<p>The whole model consists of:</p>\n\n<ul>\n<li>Patching (tokenizing)</li>\n<li>Add positional encoding</li>\n<li>Transformer Blocks (Attention + MLP)</li>\n<li>Image Reconstruction/Remapping -> Embedded Space to Pixel Space</li>\n<li>CNN Refinement</li>\n</ul>\n\n<p>Model logic:<br></p>\n\n<ul>\n<li><code>CNN(Transformer(x))</code> = Pure Transformation (skip connection = false)</li>\n<li><code>CNN(Transformer(x)) + x_input</code> = residual refinement (skip connection = true)</li>\n<li><code>Transformer(x) + CNN(Transformer(x)) + x_input</code> = global field + local correction + geometry/residual (not available yet)</li>\n</ul>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.__init__", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.__init__", "kind": "function", "doc": "<p>Init of the PhysicFormer model.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>input_channels (int): \nNumber of input image channels (e.g., 1 for grayscale, 3 for RGB).</li>\n<li>output_channels (int): \nNumber of output image channels.</li>\n<li>img_size (int): \nSize (height and width) of the input image in pixels.</li>\n<li>patch_size (int): \nSize of each square patch to split the image into. \nThe image must be divisible by this size.</li>\n<li>embedded_dim (int): \nDimension of the patch embedding (feature space size per token).</li>\n<li>num_blocks (int): \nNumber of Transformer Encoder blocks.</li>\n<li>heads (int): \nNumber of attention heads per Attention layer.</li>\n<li>mlp_dim (int): \nHidden dimension of the feed-forward (MLP) layers within each Transformer block.</li>\n<li>dropout (float): \nDropout probability for regularization applied after positional encoding and inside MLP.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">input_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">output_channels</span><span class=\"o\">=</span><span class=\"mi\">1</span>,</span><span class=\"param\">\t<span class=\"n\">img_size</span><span class=\"o\">=</span><span class=\"mi\">256</span>,</span><span class=\"param\">\t<span class=\"n\">patch_size</span><span class=\"o\">=</span><span class=\"mi\">4</span>,</span><span class=\"param\">\t<span class=\"n\">embedded_dim</span><span class=\"o\">=</span><span class=\"mi\">1026</span>,</span><span class=\"param\">\t<span class=\"n\">num_blocks</span><span class=\"o\">=</span><span class=\"mi\">8</span>,</span><span class=\"param\">\t<span class=\"n\">heads</span><span class=\"o\">=</span><span class=\"mi\">16</span>,</span><span class=\"param\">\t<span class=\"n\">mlp_dim</span><span class=\"o\">=</span><span class=\"mi\">2048</span>,</span><span class=\"param\">\t<span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.1</span></span>)</span>"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.input_channels", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.input_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.output_channels", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.output_channels", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.patch_size", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.patch_size", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.patch_embedding", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.patch_embedding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.positional_encoding", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.positional_encoding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.dropout", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.dropout", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.transformer_blocks", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.transformer_blocks", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.to_img", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.to_img", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.norm", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.norm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.refinement", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.refinement", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.get_input_channels", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.get_input_channels", "kind": "function", "doc": "<p>Returns the number of input channels used by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of input channels expected by the model.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.get_output_channels", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.get_output_channels", "kind": "function", "doc": "<p>Returns the number of output channels produced by the model.</p>\n\n<p>Returns:</p>\n\n<ul>\n<li>int: \nNumber of output channels the model generates</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.models.transformer.PhysicFormer.forward", "modulename": "image_to_image.models.transformer", "qualname": "PhysicFormer.forward", "kind": "function", "doc": "<p>Forward pass of the PhysicFormer network.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>x (torch.tensor): \nInput image tensor of shape (batch_size, input_channels, height, width).</li>\n</ul>\n\n<p>Returns:</p>\n\n<ul>\n<li>torch.tensor: \nRefined output image tensor of shape (batch_size, output_channels, height, width), \nwith values normalized to [0.0, 1.0].</li>\n</ul>\n\n<p>Notes:</p>\n\n<ul>\n<li>The output passes through a <code>sigmoid()</code> activation, ensuring all pixel values \u2208 [0, 1].</li>\n<li>Designed for physics-informed or visual reconstruction tasks where local and global consistency are important.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">x</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.scheduler", "modulename": "image_to_image.scheduler", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up", "modulename": "image_to_image.scheduler.warm_up", "kind": "module", "doc": "<p>Module to define a WarmUp Scheduler which can have a adter scheduler.</p>\n\n<p>Class:</p>\n\n<ul>\n<li>WarmUpScheduler</li>\n</ul>\n\n<p>By Tobia Ippolito</p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler", "kind": "class", "doc": "<p>Implements a learning rate scheduler with an initial warm-up phase.</p>\n\n<p>After the warm-up phase, an optional 'after scheduler' can take over\nto continue adjusting the learning rate according to another schedule.</p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.__init__", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.__init__", "kind": "function", "doc": "<p>Init WarmUp Scheduler.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>start_lr (float): \nInitial learning rate at the start of warm-up.</li>\n<li>end_lr (float): \nFinal learning rate at the end of warm-up.</li>\n<li>optimizer (torch.optim.Optimizer): \nOptimizer whose learning rate will be updated.</li>\n<li>scheduler (torch.optim.lr_scheduler._LRScheduler, optional): \nScheduler to apply after warm-up.</li>\n<li>step_duration (int): \nNumber of steps over which to increase the learning rate.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">start_lr</span>, </span><span class=\"param\"><span class=\"n\">end_lr</span>, </span><span class=\"param\"><span class=\"n\">optimizer</span>, </span><span class=\"param\"><span class=\"n\">scheduler</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">step_duration</span><span class=\"o\">=</span><span class=\"mi\">1000</span></span>)</span>"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.start_lr", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.start_lr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.end_lr", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.end_lr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.current_lr", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.current_lr", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.step_duration", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.step_duration", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.optimizer", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.optimizer", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.scheduler", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.scheduler", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.current_step", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.current_step", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.lrs", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.lrs", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.step", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.step", "kind": "function", "doc": "<p>Performs a single step in the scheduler. \nDuring warm-up, linearly increases the learning rate. \nAfter warm-up, delegates to the optional after-scheduler.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.get_last_lr", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.get_last_lr", "kind": "function", "doc": "<p>Returns the most recently applied learning rate as a list.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.state_dict", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.state_dict", "kind": "function", "doc": "<p>Returns a dictionary with the current step and after-scheduler state.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "image_to_image.scheduler.warm_up.WarmUpScheduler.load_state_dict", "modulename": "image_to_image.scheduler.warm_up", "qualname": "WarmUpScheduler.load_state_dict", "kind": "function", "doc": "<p>Loads the scheduler state from a given dictionary.</p>\n\n<p>Parameter:</p>\n\n<ul>\n<li>state (dict): \nDictionary containing 'current_step' and optional 'after_scheduler' state.</li>\n</ul>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();