{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cce372f",
   "metadata": {},
   "source": [
    "# Ray-Tracing Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PhysGen Dataset\n",
    "\n",
    "See:\n",
    "- https://huggingface.co/datasets/mspitzna/physicsgen\n",
    "- https://arxiv.org/abs/2503.05333\n",
    "- https://github.com/physicsgen/physicsgen\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "def resize_tensor_to_divisible_by_14(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Resize a tensor to the next smaller (H, W) divisible by 14.\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor of shape (C, H, W) or (B, C, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Resized tensor\n",
    "    \"\"\"\n",
    "    if tensor.dim() == 3:\n",
    "        c, h, w = tensor.shape\n",
    "        new_h = h - (h % 14)\n",
    "        new_w = w - (w % 14)\n",
    "        return F.interpolate(tensor.unsqueeze(0), size=(new_h, new_w), mode='bilinear', align_corners=False).squeeze(0)\n",
    "    \n",
    "    elif tensor.dim() == 4:\n",
    "        b, c, h, w = tensor.shape\n",
    "        new_h = h - (h % 14)\n",
    "        new_w = w - (w % 14)\n",
    "        return F.interpolate(tensor, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Tensor must be 3D (C, H, W) or 4D (B, C, H, W)\")\n",
    "\n",
    "\n",
    "class PhysGenDataset(Dataset):\n",
    "\n",
    "    def __init__(self, variation=\"sound_baseline\", mode=\"train\", input_type=\"osm\", output_type=\"standard\"):\n",
    "        \"\"\"\n",
    "        Loads PhysGen Dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - variation : str\n",
    "            Chooses the used dataset variant: sound_baseline, sound_reflection, sound_diffraction, sound_combined.\n",
    "        - mode : str\n",
    "            Can be \"train\", \"test\", \"eval\".\n",
    "        - input_type : str\n",
    "            Defines the used Input -> \"osm\", \"base_simulation\"\n",
    "        - output_type : str\n",
    "            Defines the Output -> \"standard\", \"complex_only\"\n",
    "        \"\"\"\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "        # get data\n",
    "        self.dataset = load_dataset(\"mspitzna/physicsgen\", name=variation, trust_remote_code=True)\n",
    "        # print(\"Keys:\", self.dataset.keys())\n",
    "        self.dataset = self.dataset[mode]\n",
    "        \n",
    "        self.input_type = input_type\n",
    "        self.output_type = output_type\n",
    "        if self.input_type == \"base_simulation\" or self.output_type == \"complex_only\":\n",
    "            self.basesimulation_dataset = load_dataset(\"mspitzna/physicsgen\", name=\"sound_baseline\", trust_remote_code=True)\n",
    "            self.basesimulation_dataset = self.basesimulation_dataset[mode]\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts [0,255] PIL image to [0,1] FloatTensor\n",
    "        ])\n",
    "        print(f\"PhysGen ({variation}) Dataset for {mode} got created\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        # print(sample)\n",
    "        # print(sample.keys())\n",
    "        if self.input_type == \"base_simulation\":\n",
    "            input_img = self.basesimulation_dataset[idx][\"soundmap\"]\n",
    "        else:\n",
    "            input_img = sample[\"osm\"]  # PIL Image\n",
    "        target_img = sample[\"soundmap\"]  # PIL Image\n",
    "\n",
    "        input_img = self.transform(input_img)\n",
    "        target_img = self.transform(target_img)\n",
    "\n",
    "        # Fix real image size 512x512 > 256x256\n",
    "        input_img = F.interpolate(input_img.unsqueeze(0), size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        input_img = input_img.squeeze(0)\n",
    "        # target_img = target_img.unsqueeze(0)\n",
    "\n",
    "        # # change size\n",
    "        # input_img = resize_tensor_to_divisible_by_14(input_img)\n",
    "        # target_img = resize_tensor_to_divisible_by_14(target_img)\n",
    "\n",
    "        # add fake rgb\n",
    "        # if input_img.shape[0] == 1:  # shape (B, 1, H, W)\n",
    "        #     input_img = input_img.repeat(3, 1, 1)  # make it (B, 3, H, W)\n",
    "\n",
    "        if self.output_type == \"complex_only\":\n",
    "            base_simulation_img = self.transform(self.basesimulation_dataset[idx][\"soundmap\"])\n",
    "            # base_simulation_img = resize_tensor_to_divisible_by_14(self.transform(self.basesimulation_dataset[idx][\"soundmap\"]))\n",
    "            # target_img = torch.abs(target_img[0] - base_simulation_img[0])\n",
    "            target_img = target_img[0] - base_simulation_img[0]\n",
    "            target_img = target_img.unsqueeze(0)\n",
    "            target_img *= -1\n",
    "\n",
    "        return input_img, target_img, idx\n",
    "\n",
    "\n",
    "\n",
    "def get_dataloader(mode='train', variation=\"sound_reflection\", input_type=\"osm\", output_type=\"complex_only\", shuffle=True):\n",
    "    dataset = PhysGenDataset(mode=mode, variation=variation, input_type=input_type, output_type=output_type)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=shuffle, num_workers=1)\n",
    "\n",
    "\n",
    "\n",
    "def get_image(mode='train', variation=\"sound_reflection\", input_type=\"osm\", output_type=\"complex_only\", shuffle=True, \n",
    "              return_output=False, as_numpy_array=True):\n",
    "    dataset = PhysGenDataset(mode=mode, variation=variation, input_type=input_type, output_type=output_type)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=shuffle, num_workers=1)\n",
    "    cur_data = next(iter(loader))\n",
    "    input_ = cur_data[0]\n",
    "    output_ = cur_data[1]\n",
    "\n",
    "    if as_numpy_array:\n",
    "        input_ = input_.detach().cpu().numpy()\n",
    "        output_ = output_.detach().cpu().numpy()\n",
    "\n",
    "        # remove batch channel\n",
    "        input_ = np.squeeze(input_, axis=0)\n",
    "        output_ = np.squeeze(output_, axis=0)\n",
    "\n",
    "        if len(input_.shape) == 3:\n",
    "            input_ = np.squeeze(input_, axis=0)\n",
    "            output_ = np.squeeze(output_, axis=0)\n",
    "\n",
    "        # opencv format\n",
    "        # if np.issubdtype(img.dtype, np.floating):\n",
    "        #     img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        #     img = (img * 255).astype(np.uint8)\n",
    "        input_ = np.transpose(input_, (1, 0))\n",
    "        output_ = np.transpose(output_, (1, 0))\n",
    "\n",
    "\n",
    "    result = input_\n",
    "    if return_output:\n",
    "        result = [input_, output_]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def save_dataset(output_real_path, output_osm_path, \n",
    "                 variation, input_type, output_type,\n",
    "                 data_mode, \n",
    "                 info_print=False, progress_print=True):\n",
    "    # Clearing\n",
    "    if os.path.exists(output_osm_path) and os.path.isdir(output_osm_path):\n",
    "        shutil.rmtree(output_osm_path)\n",
    "        os.makedirs(output_osm_path)\n",
    "        print(f\"Cleared {output_osm_path}.\")\n",
    "    else:\n",
    "        os.makedirs(output_osm_path)\n",
    "        print(f\"Created {output_osm_path}.\")\n",
    "\n",
    "    if os.path.exists(output_real_path) and os.path.isdir(output_real_path):\n",
    "        shutil.rmtree(output_real_path)\n",
    "        os.makedirs(output_real_path)\n",
    "        print(f\"Cleared {output_real_path}.\")\n",
    "    else:\n",
    "        os.makedirs(output_real_path)\n",
    "        print(f\"Created {output_real_path}.\")\n",
    "    \n",
    "    # Load Dataset\n",
    "    dataset = PhysGenDataset(mode=data_mode, variation=variation, input_type=input_type, output_type=output_type)\n",
    "    data_len = len(dataset)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Save Dataset\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # if progress_print:\n",
    "            # print(f'Progress {i+1}/{data_len}')\n",
    "            # prime.get_progress_bar(total=data_len, progress=i+1, \n",
    "            #                        should_clear=True, left_bar_char='|', right_bar_char='|', \n",
    "            #                        progress_char='#', empty_char=' ', \n",
    "            #                        front_message='Physgen Data Loading', back_message='', size=15)\n",
    "\n",
    "        input_img, target_img, idx = data\n",
    "        idx = idx[0].item() if isinstance(idx, torch.Tensor) else idx\n",
    "\n",
    "        # forward_img = inference_forward(input_img, model, DEVICE)\n",
    "\n",
    "        if info_print:\n",
    "            # print(f\"Prediction shape [forward]: {forward_img.shape}\")\n",
    "            print(f\"Prediction shape [osm]: {input_img.shape}\")\n",
    "            print(f\"Prediction shape [target]: {target_img.shape}\")\n",
    "\n",
    "            print(f\"OSM Info:\\n    -> shape: {input_img.shape}\\n    -> min: {input_img.min()}, max: {input_img.max()}\")\n",
    "\n",
    "        # Transform to Numpy\n",
    "        # pred_img = forward_img.squeeze(2)\n",
    "        # if not (0 <= pred_img.min() <= 255 and 0 <= pred_img.max() <=255):\n",
    "        #     raise ValueError(f\"Prediction has values out of 0-256 range => min:{pred_img.min()}, max:{pred_img.max()}\")\n",
    "        # if pred_img.max() <= 1.0:\n",
    "        #     pred_img *= 255\n",
    "        # pred_img = pred_img.astype(np.uint8)\n",
    "\n",
    "        real_img = target_img.squeeze(0).cpu().squeeze(0).detach().numpy()\n",
    "        if not (0 <= real_img.min() <= 255 and 0 <= real_img.max() <=255):\n",
    "            raise ValueError(f\"Real target has values out of 0-256 range => min:{real_img.min()}, max:{real_img.max()}\")\n",
    "        if info_print:\n",
    "            print( f\"\\nReal target has values out of 0-256 range => min:{real_img.min()}, max:{real_img.max()}\")\n",
    "        if real_img.max() <= 1.0:\n",
    "            real_img *= 255\n",
    "        if info_print:\n",
    "            print( f\"Real target has values out of 0-256 range => min:{real_img.min()}, max:{real_img.max()}\")\n",
    "        real_img = real_img.astype(np.uint8)\n",
    "        if info_print:\n",
    "            print( f\"Real target has values out of 0-256 range => min:{real_img.min()}, max:{real_img.max()}\")\n",
    "\n",
    "        if len(input_img.shape) == 4:\n",
    "            osm_img = input_img[0, 0].cpu().detach().numpy()\n",
    "        else:\n",
    "            osm_img = input_img[0].cpu().detach().numpy()\n",
    "        if not (0 <= osm_img.min() <= 255 and 0 <= osm_img.max() <=255):\n",
    "            raise ValueError(f\"Real target has values out of 0-256 range => min:{osm_img.min()}, max:{osm_img.max()}\")\n",
    "        if osm_img.max() <= 1.0:\n",
    "            osm_img *= 255\n",
    "        osm_img = osm_img.astype(np.uint8)\n",
    "\n",
    "        if info_print:\n",
    "            print(f\"OSM Info:\\n    -> shape: {osm_img.shape}\\n    -> min: {osm_img.min()}, max: {osm_img.max()}\")\n",
    "\n",
    "        # Save Results\n",
    "        file_name = f\"physgen_{idx}.png\"\n",
    "\n",
    "        # save pred image\n",
    "        # save_img = os.path.join(output_pred_path, file_name)\n",
    "        # cv2.imwrite(save_img, pred_img)\n",
    "        # print(f\"    -> saved pred at {save_img}\")\n",
    "\n",
    "        # save real image\n",
    "        save_img = os.path.join(output_real_path, \"target_\"+file_name)\n",
    "        cv2.imwrite(save_img, real_img)\n",
    "        if info_print:\n",
    "            print(f\"    -> saved real at {save_img}\")\n",
    "\n",
    "        # save osm image\n",
    "        save_img = os.path.join(output_osm_path, \"input_\"+file_name)\n",
    "        cv2.imwrite(save_img, osm_img)\n",
    "        if info_print:\n",
    "            print(f\"    -> saved osm at {save_img}\")\n",
    "    print(f\"\\nSuccessfull saved {data_len} datapoints into {os.path.abspath(output_real_path)} & {os.path.abspath(output_osm_path)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ef5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import img_phy_sim as ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"train\"\n",
    "reflexion_traces = 360\n",
    "\n",
    "ground_path = \"./rays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PhysGenDataset(mode=mode, variation=\"sound_reflection\", input_type=\"osm\", output_type=\"standart\")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "reflexion_steps = 360/reflexion_traces\n",
    "\n",
    "path = os.path.join(ground_path, mode, str(reflexion_traces))  # \"rays.txt\"\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "shutil.rmtree(path)\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "for input_, _, idx in tqdm(loader):\n",
    "    idx = idx.item()\n",
    "    input_ = input_.detach().cpu().numpy()\n",
    "\n",
    "    # remove batch channel\n",
    "    input_ = np.squeeze(input_, axis=0)\n",
    "\n",
    "    if len(input_.shape) == 3:\n",
    "        input_ = np.squeeze(input_, axis=0)\n",
    "\n",
    "    input_ = np.transpose(input_, (1, 0))\n",
    "    \n",
    "    # calc ray tracing\n",
    "    rays = ips.ray_tracing.trace_beams(rel_position=[0.5, 0.5], \n",
    "                                        img_src=input_, \n",
    "                                        directions_in_degree=ips.math.get_linear_degree_range(step_size=reflexion_steps),\n",
    "                                        wall_values=None, \n",
    "                                        wall_thickness=1,\n",
    "                                        img_border_also_collide=False,\n",
    "                                        reflexion_order=3,\n",
    "                                        should_scale_rays=True,\n",
    "                                        should_scale_img=True)\n",
    "\n",
    "    # save ray tracing\n",
    "    save_path = os.path.join(path, f\"rays_[{idx}].txt\")\n",
    "    ips.ray_tracing.save(path=save_path, rays=rays)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
